{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d3b8d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai_mini_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m)))\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TaskType\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrecommend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m recommend_graph\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_teddynote\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraphs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m visualize_graph\n\u001b[32m      9\u001b[39m visualize_graph(recommend_graph, xray=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\projectree\\projectree-inference\\ProjecTree-Inference\\app\\agents\\recommend\\graph.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrecommend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecommendationState\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrecommend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrouters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m route_task, should_retry\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrecommend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnodes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     frontend_expert,\n\u001b[32m      6\u001b[39m     backend_expert,\n\u001b[32m      7\u001b[39m     advance_expert,\n\u001b[32m      8\u001b[39m     expert_route,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrecommend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnodes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeedback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m route_feedback\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrecommend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnodes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tech_stack_integrator\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SSAFY\\projectree\\projectree-inference\\ProjecTree-Inference\\app\\agents\\recommend\\nodes\\experts.py:28\u001b[39m\n\u001b[32m     25\u001b[39m load_dotenv()\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# [변경] 메인 모델을 gpt-5.2로 교체\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m llm = \u001b[43mopenai_mini_llm\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# [변경] 검색 툴을 메인 에이전트가 직접 사용\u001b[39;00m\n\u001b[32m     30\u001b[39m tools = [restricted_search, url_validator]\n",
      "\u001b[31mNameError\u001b[39m: name 'openai_mini_llm' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from app.agents.enums import TaskType\n",
    "from app.agents.recommend.graph import recommend_graph\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(recommend_graph, xray=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57417830",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\projectree\\projectree-inference\\ProjecTree-Inference\\app\\agents\\tools\\search.py:60: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  search_tool = TavilySearchResults(\n"
     ]
    }
   ],
   "source": [
    "from app.core.log import langfuse_handler\n",
    "\n",
    "ouput = recommend_graph.invoke({\n",
    "    \"workspace_id\":1,\n",
    "    \"task_type\": TaskType.BACKEND, \n",
    "    \"node_name\": \"N : M 채팅 기능 구현\", \n",
    "    \"node_description\": \"\"\"\n",
    "    1. 사용자가 입력한 채팅 메시지를 처리하고 응답을 생성\n",
    "    2. 사용자의 채팅 메시지가 포함된 대화를 데이터베이스에 저장\n",
    "    \"\"\"\n",
    "}, config={\"callbacks\": [langfuse_handler]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a274227f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"techs\": [\n",
      "    {\n",
      "      \"id\": \"317\",\n",
      "      \"name\": \"websocket\",\n",
      "      \"advantage\": \"실시간성이 가장 좋고(지연 최소), 채팅처럼 **양방향 상호작용**에 최적입니다. 연결이 유지되므로 요청/응답 오버헤드가 적어 메시지 빈도가 높을수록 유리합니다.\",\n",
      "      \"disadvantage\": \"연결을 장시간 유지하므로 서버 리소스/커넥션 관리가 필요합니다(스케일 아웃, 세션 관리, 로드밸런싱, 장애 시 재연결/재전송 설계 등). 운영 난이도가 HTTP 기반 방식보다 높습니다.\",\n",
      "      \"description\": \"클라이언트와 서버가 **한 번 연결(Handshake)** 한 뒤, 연결을 유지하면서 **양방향(duplex)으로 메시지를 즉시 푸시**하는 방식입니다. N:M 채팅은 보통 (1) 채팅방(room) 단위로 연결된 세션을 묶고 (2) 누군가 메시지를 보내면 같은 room에 속한 모든 세션에 브로드캐스트합니다. \\n\\nDB 저장은 보통 흐름이 `메시지 수신 → 유효성/권한 체크 → DB에 메시지(및 대화 메타) 저장 → 해당 방 구독자들에게 전송` 순서로 설계합니다. (전송보다 저장을 먼저 하면 ‘전송됐는데 저장이 안 됨’ 같은 불일치를 줄이기 좋습니다.)\",\n",
      "      \"ref\": \"https://changbroblog.tistory.com/221\",\n",
      "      \"recommendation_score\": 5\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"3461\",\n",
      "      \"name\": \"server-sent-events\",\n",
      "      \"advantage\": \"WebSocket보다 구현/디버깅이 단순한 편이고, HTTP 기반이라 인프라(프록시/게이트웨이) 친화적입니다. 기본적으로 자동 재연결을 지원해 실서비스에서 다루기 편합니다.\",\n",
      "      \"disadvantage\": \"기본적으로 **서버→클라이언트 단방향**이라 ‘메시지 보내기’는 별도 HTTP 호출이 필요합니다. 아주 활발한 채팅(초당 이벤트가 많음)이나 양방향 상호작용이 많을수록 WebSocket 대비 한계가 빨리 옵니다.\",\n",
      "      \"description\": \"브라우저 표준인 **EventSource 기반 단방향 스트리밍**으로, 서버→클라이언트 방향으로 이벤트를 계속 밀어줍니다. 채팅에서 ‘상대 메시지 수신’은 SSE로 받고, ‘내 메시지 전송’은 일반 HTTP POST로 보내는 식으로 조합합니다.\\n\\nN:M 구현은 `room별 SSE 연결(또는 사용자별 연결)`을 유지하고, 새 메시지가 DB에 저장되면 해당 room의 참여자들에게 이벤트를 푸시합니다. (자동 재연결 기능이 있어 끊김 복구가 비교적 쉽습니다.)\",\n",
      "      \"ref\": \"https://velog.io/@jooooo/Web-WebSocket-Server-Sent-EventsSSE-%EB%B9%84%EA%B5%90\",\n",
      "      \"recommendation_score\": 4\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"5160\",\n",
      "      \"name\": \"long-polling\",\n",
      "      \"advantage\": \"추가 프로토콜 없이 HTTP만으로 구현 가능해 가장 단순하고, 기존 REST 인프라에 얹기 쉽습니다. 초기 PoC/소규모 트래픽에 현실적인 선택지입니다.\",\n",
      "      \"disadvantage\": \"요청이 자주 발생해 네트워크/서버 오버헤드가 커지기 쉽고(특히 사용자가 많을 때), 타임아웃/재시도/중복응답 같은 엣지 케이스가 늘어납니다. 실시간성도 WebSocket/SSE 대비 불리합니다.\",\n",
      "      \"description\": \"클라이언트가 서버에 요청을 보낸 뒤, 서버는 **새 메시지가 생길 때까지(또는 타임아웃까지) 응답을 보류**했다가 이벤트가 생기면 응답하는 방식입니다. 응답을 받으면 클라이언트는 즉시 다음 long-poll 요청을 다시 보냅니다.\\n\\nN:M 채팅에서는 `방 참여자별로 마지막으로 읽은 메시지 ID/시간`을 기준으로, long-poll 요청 시점까지 새로 저장된 메시지를 DB에서 조회해 내려줍니다. 메시지 전송은 일반 HTTP POST로 처리합니다.\",\n",
      "      \"ref\": \"https://maramincho.tistory.com/189\",\n",
      "      \"recommendation_score\": 3\n",
      "    }\n",
      "  ],\n",
      "  \"comparison\": \"## 기술 비교 보고서\\n\\n| 기준 | WebSocket | Server-Sent Events(SSE) | Long Polling |\\n|---|---|---|---|\\n| 통신 방향 | 양방향 | 서버→클라(단방향) + 클라→서버는 HTTP로 보완 | HTTP 요청/응답 기반(준-실시간) |\\n| 실시간성/지연 | 최상 | 좋음 | 보통(타임아웃/폴 주기에 영향) |\\n| 구현 난이도(주니어 관점) | 중~상(연결/세션/브로드캐스트) | 중(수신 단순, 송신은 HTTP) | 하(REST 확장 느낌) |\\n| 운영/스케일링 난이도 | 높음(커넥션/로드밸런싱/장애복구) | 중(HTTP 친화적, 연결 유지 이슈는 존재) | 중~높음(요청 폭증/타임아웃 관리) |\\n| N:M 채팅 적합도 | 매우 높음(정석) | 중~높음(‘수신 푸시’ 중심이면 적합) | 낮~중(소규모/임시 대안) |\\n\\n### 선택 가이드(프로젝트 관리 툴 기준)\\n- **사용자 간 실시간 대화가 핵심 기능**이고, 추후 사용자/메시지량이 늘 가능성이 있으면: **WebSocket**\\n- “실시간 알림/메시지 수신”이 주 목적이고, 구현/운영 단순성을 더 원하면: **SSE(+HTTP POST 전송)**\\n- 짧은 기간 MVP/PoC, 트래픽이 작고 인프라 단순화를 최우선이면: **Long Polling**\\n\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# indent=4를 주어 예쁘게 출력\n",
    "json_string = ouput['tech_list'].model_dump_json(indent=2)\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afd66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectree",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
